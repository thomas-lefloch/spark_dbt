TP Introduction Pyspark et DBT


Comprendre comment utiliser PySpark pour transformer des données massives et DBT pour modéliser ces données dans un entrepôt.

Vous travaillez pour une société de ventes à distance.
Votre mission est de :
- Nettoyer et transformer des données avec PySpark.
- Charger ces données dans une base SQL (DuckDB ou PostgreSQL au choix).
- Construire un modèle en étoile simple avec DBT.
- Répondre à certaines questions métier via DBT.

Dataset :

Cet ensemble de données contient des informations sur les ventes réalisées par une entreprise, chaque ligne correspond à une vente.



Nettoyage des données avec PySpark

Voici un exemple de code :

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("TP_Ventes").getOrCreate()
df = spark.read.csv("ventes.csv", header=True, inferSchema=True)

Mettez toutes les valeurs string en lowercase.
Eliminez les lignes où au moins une ligne est manquante
Eliminez les lignes où la date d'achat est extravagante
Globalement, éliminez les lignes où les cellules présentent des anomalies (chaîne de caractères à la place de nombre par exemple)
Si l'âge est négatif, le mettre en positif.
Calculez le montant total pour chaque vente

Lorsque vous aurez terminé, je vous propose d'intégrer votre CSV nettoyé à une table DuckDB (ex: ventes_clean.duckdb)


A partir de la source DuckDB (ventes_clean.duckdb), construisez un schéma en étoile, utilisez la commande dbt init (installez dbt avant cela).

Workflow dbt

1. Configurer profiles.yml pour pointer vers la base DuckDB :

ventes_dbt:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: ventes_clean.duckdb

2. Créer un modèle de staging (stg_ventes.sql) qui lit la table brute depuis DuckDB.
3. Créer les dimensions (dim_client.sql, dim_produit.sql, dim_magasin.sql, dim_temps.sql).
4. Créer la table de faits (fact_ventes.sql).
5. Exécuter dbt run pour matérialiser l’étoile.



Questions métier

Écrivez les requêtes analytiques pour répondre aux questions métier suivantes :

Analyse temporelle
Quel est le chiffre d’affaires mensuel de la boutique ?
Quels sont les mois les plus rentables de l’année ?
Quelle est l’évolution du panier moyen (montant_total / transaction) au fil du temps ?

Analyse produit
Quels sont les produits les plus vendus en quantité ?
Quels produits génèrent le plus de chiffre d’affaires ?
Quelles catégories de produits sont les plus populaires ?

Analyse client
Quelle est la répartition des ventes par tranche d’âge (10 ans) ?
Quelle ville ou région génère le plus de ventes ?







Bonus : 
Analyse magasin / canal de vente
Quelle part de ventes vient des boutiques physiques vs de l’e-shop ?
Quel magasin physique génère le plus de transactions ?

Analyse croisée
Quels produits sont achetés par les jeunes clients (<30 ans) ?
Quelles catégories sont les plus vendues sur le e-shop vs en boutique ?
Existe-t-il une saisonnalité (ex: pics sur certains mois/produits) ?


Notions à travailler :

- Quel le modèle en étoile ?
- Avec PySpark qu'est-ce qu'un RDD ?
- Avec PySpark qu'est-ce qu'une partition ?
- Avec PySpark qu'est-ce qu'un executor ?


Ressources :
https://sparkbyexamples.com/pyspark-tutorial/


Livrable : Notebook individuel.
Planning prévisionnel

Jour 1
Rechercher sur PySpark

Jour 2
Exploration et nettoyage avec Pyspark

Jour 3
Nettoyage avec Pyspark

Jour 4
Constitution de l’entrepôt avec DBT
Schématisation en étoile
Traitement des requêtes analytiques métier


Jour 5
Traitement des requêtes analytiques métier

Jour 6
Peaufinage
